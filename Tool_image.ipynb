{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xhQOGGEEcFV"
      },
      "source": [
        "## This tool is offered by Nao \n",
        "\n",
        "![Image](https://static.wixstatic.com/media/88463b_4528450409ad4ff182fd5ac461a98d2f~mv2.png/v1/fill/w_192,h_68,al_c,q_85,usm_0.66_1.00_0.01,enc_auto/88463b_4528450409ad4ff182fd5ac461a98d2f~mv2.png \"markdown\")\n",
        "\n",
        "www.nao.school\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsN0DEKznT3o"
      },
      "source": [
        "# Stable Diffusion\n",
        "\n",
        "Stable Diffusion is a text-to-image model. We'll use this model to create our unique Xmas image ðŸŽ…\n",
        "\n",
        "## Setup GPU\n",
        "\n",
        "First of all, we'll use <b>GPU runtime</b> to run this notebook, so inference is much faster.\n",
        "\n",
        "âš ï¸ Use the `Runtime`  menu above and select `Change runtime type`.\n",
        "\n",
        "## Setup libraries\n",
        "\n",
        "Then, you just have to run the cell bellow to install some very usefull libraries to use Stable Diffusion\n",
        "\n",
        "The `%%` characters for Colab built-in magic commands\n",
        "\n",
        "This `!` character means that we are running shell commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7zNsYKYmMog"
      },
      "outputs": [],
      "source": [
        "# hide the outputs\n",
        "%%capture\n",
        "\n",
        "!pip install timm # huggingface_hub \n",
        "!pip install transformers scipy ftfy\n",
        "!pip install diffusers==0.8.0\n",
        "!pip install stable_diffusion_videos\n",
        "!pip install \"ipywidgets>=7,<8\"\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiVAEgvpI3o"
      },
      "source": [
        "## Import\n",
        "\n",
        "We import some useful libraries with Colab and computer vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QzNeiAdmM0H"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "from stable_diffusion_videos import StableDiffusionWalkPipeline, Interface\n",
        "\n",
        "from torch import autocast, float16, Generator\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZJe8QykmifQ"
      },
      "outputs": [],
      "source": [
        "# then, we'll load the model Stable Diffusion\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=float16)\n",
        "pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHc4ruy0UcaK"
      },
      "source": [
        "`num_inference_steps`: you can change the number of inference steps using the num_inference_steps argument. In general, results are better the more steps you use, however the more steps, the longer the generation takes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9IuQdzGm0kY"
      },
      "outputs": [],
      "source": [
        "generator_cat = Generator(\"cuda\").manual_seed(32) # remove manual_seed and the parameter generator to generate a random image\n",
        "\n",
        "prompt = \"a photograph of a very cute cat\"\n",
        "with autocast(\"cuda\"):\n",
        "  cat_image = pipe(prompt, num_inference_steps=50, generator=generator_cat, height=512, width=512).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "# to save it \n",
        "cat_image.save(\"cat.png\")\n",
        "\n",
        "# or if you're in a google colab you can directly display it with \n",
        "cat_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-u4wt3X4gq7"
      },
      "source": [
        "## Well done ðŸŽ‰ you have created your first image !\n",
        "\n",
        "Now it's time to change the sentence of the prompt by your own to create another image ðŸ¤“\n",
        "\n",
        "Some short examples:\n",
        "- \"Rubix Cube in Van Gogh style\";\n",
        "- \"Eiffel Tower in kaleidoscope style\";\n",
        "- \"A drawing of two dogs\";\n",
        "- \"a minimalist pine tree\";\n",
        "- \"Feminist in Picasso style\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTbFLSn_YfMM"
      },
      "outputs": [],
      "source": [
        "prompt = \"a Rubix Cube in Pablo Picasso style\"\n",
        "with autocast(\"cuda\"):\n",
        "  image = pipe(prompt, num_inference_steps=50, height=512, width=512).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMj26bmad03z"
      },
      "source": [
        "## Now we are ready to create lots of images! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gek_CC5CBerC"
      },
      "outputs": [],
      "source": [
        "# this function allow us to display multiple images\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "num_cols = 3 # how many columns of images do you need ?\n",
        "num_rows = 4 # how many rows of images do you need ?\n",
        "\n",
        "prompt = [\"a pile of gifts\" ] * num_cols\n",
        "\n",
        "all_images = []\n",
        "for i in range(num_rows):\n",
        "  with autocast(\"cuda\"):\n",
        "    images = pipe(prompt, num_inference_steps=50, height=512, width=512).images\n",
        "  all_images.extend(images)\n",
        "\n",
        "grid = image_grid(all_images, rows=num_rows, cols=num_cols)\n",
        "grid"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
